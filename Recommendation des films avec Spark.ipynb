{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2322d31c",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques nécessaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e39717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,  functions as F\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import col, udf, concat_ws, regexp_replace\n",
    "from pyspark.sql.functions import concat, col, lit, lower, split\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer,HashingTF\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer                                                               \n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9277391",
   "metadata": {},
   "source": [
    "# Initialiser la session Spark avec des paramètres ajustés :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c14ec285",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"MovieRecommendations\") \\\n",
    "                  .set(\"spark.executor.memory\", \"6g\") \\\n",
    "                  .set(\"spark.driver.memory\", \"6g\") \\\n",
    "                  .set(\"spark.network.timeout\", \"600s\") \\\n",
    "                  .set(\"spark.executor.heartbeatInterval\", \"60s\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657b259",
   "metadata": {},
   "source": [
    "# l'importation du fichier CSV :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "475bf91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read file into dataFrame using automatically inferred schema\n",
    "data = spark.read.csv('C:\\\\Users\\\\HP\\\\Downloads\\\\dataFinale', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2cd09",
   "metadata": {},
   "source": [
    "# Finalisation du nettoyage des données avec PySpark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b09ac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommer la colonne '\\N' en 'duration' avec une chaîne brute\n",
    "data_renamed = data.withColumnRenamed(r'\\N', 'duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77965655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs manquantes par des espaces vides dans la colonne \"year\". Comme l'année d'apparition du film n'est pas très importante, nous choisissons de remplacer les valeurs manquantes par des espaces vides\n",
    "data_filled = data_renamed.fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06a1ba00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+--------------+----+-----+--------+--------+------+------+-----------+\n",
      "|imdb_title_id|title|original_title|year|genre|duration|director|writer|actors|description|\n",
      "+-------------+-----+--------------+----+-----+--------+--------+------+------+-----------+\n",
      "|            0|    1|             1|   0|    0|       0|       0|     0|     0|          0|\n",
      "+-------------+-----+--------------+----+-----+--------+--------+------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "\n",
    "# Compter les valeurs manquantes dans chaque colonne\n",
    "missing_values = data_filled.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data_filled.columns])\n",
    "\n",
    "# Afficher les résultats\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb642db",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_convertir=['duration','description','year','genre','director','writer','actors']\n",
    "# Convertir les colonnes en StringType\n",
    "for colonne in colonnes_a_convertir:\n",
    "    data_filled=data_filled.withColumn(colonne, col(colonne).cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39c9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction pour supprimer les espaces dans les chaines de caracteres\n",
    "def remove_space(text):\n",
    "    if text is not None:\n",
    "        return text.replace(\" \",\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d64972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on a trasformer la fonction remove_space vers une fonction udf de Spark:\n",
    "remove_space_udf=udf(remove_space,StringType())\n",
    "# on a utiliser la fonction  remove_space_udf pour supprimer les espaces dans les chaines de caracteres\n",
    "data=data_filled.withColumn('genre',remove_space_udf(col('genre')))\n",
    "data=data.withColumn('actors',remove_space_udf(col('actors')))\n",
    "data=data.withColumn('writer',remove_space_udf(col('writer')))\n",
    "data=data.withColumn('director',remove_space_udf(col('director')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb0de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons créé une nouvelle colonne 'tags' qui contient la concaténation des colonnes 'actors', 'description', 'director', 'writer', 'genre', 'duration' et 'year'.\n",
    "data = data.withColumn('tags', concat(col('description'), lit(' '), col('actors'),lit(' '),col('director'), lit(' '), col('writer'),lit(' '),col('genre'), lit(' '), col('duration'), lit(' '), col('year')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "486423bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer la colonne tags en miniscule\n",
    "data= data.withColumn('tags', lower(col('tags')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec249ea5",
   "metadata": {},
   "source": [
    "# Préparation des données avant la création de la fonction de recommandation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a3c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               words|\n",
      "+--------------------+\n",
      "|[the, adventures,...|\n",
      "|[true, story, of,...|\n",
      "|[two, men, of, hi...|\n",
      "|[the, fabled, que...|\n",
      "|[loosely, adapted...|\n",
      "|[an, account, of,...|\n",
      "|[the, story, of, ...|\n",
      "|[\"an, epic, itali...|\n",
      "|[the, movie, depi...|\n",
      "|[richard, of, glo...|\n",
      "|[after, dr., frie...|\n",
      "|[inspector, juve,...|\n",
      "|[single, mother, ...|\n",
      "|[in, part, two, o...|\n",
      "|[leslie, swayne, ...|\n",
      "|[an, army, pilot,...|\n",
      "|[after, a, body, ...|\n",
      "|[john, howard, pa...|\n",
      "|[balduin, a, stud...|\n",
      "|[a, woman, with, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer pour transformer 'tags' en 'words'\n",
    "tokenizer = Tokenizer(inputCol=\"tags\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(data)\n",
    "\n",
    "# StopWordsRemover pour enlever les mots vides\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_data = remover.transform(words_data)\n",
    "# affichage de la colonne words\n",
    "filtered_data.select('words').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "969b77c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      filtered_words|\n",
      "+--------------------+\n",
      "|[adventures, fema...|\n",
      "|[true, story, not...|\n",
      "|[two, men, high, ...|\n",
      "|[fabled, queen, e...|\n",
      "|[loosely, adapted...|\n",
      "|[account, life, j...|\n",
      "|[story, madame, d...|\n",
      "|[\"an, epic, itali...|\n",
      "|[movie, depicts, ...|\n",
      "|[richard, glouces...|\n",
      "|[dr., friedrich's...|\n",
      "|[inspector, juve,...|\n",
      "|[single, mother, ...|\n",
      "|[part, two, louis...|\n",
      "|[leslie, swayne, ...|\n",
      "|[army, pilot, vis...|\n",
      "|[body, disappears...|\n",
      "|[john, howard, pa...|\n",
      "|[balduin, student...|\n",
      "|[woman, aid, poli...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# affichage de la colonne 'filtered_words'\n",
    "filtered_data.select('filtered_words').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9831df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des TF pour convertir des données textuelles en vecteurs de caractéristiques\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurized_data = hashing_tf.transform(filtered_data)\n",
    "# Calcul des IDF (Inverse Document Frequency) pour réduire le poids des termes qui apparaissent fréquemment dans le corpus, car ils sont moins discriminants pour la classification ou la recherche d'informations.\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized_data)\n",
    "vectorized_data= idf_model.transform(featurized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1063e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,[303,1321,2345,2456,4307,5245,6526,8415],[4.490668502017955,13.584378397912428,6.211520001822959,5.172169177191576,6.6169851099311225,5.151335090288734,4.4561281767657785,5.483281501451743])\n"
     ]
    }
   ],
   "source": [
    "# Récupérer la première ligne du DataFrame transformé\n",
    "first_row = vectorized_data.first()\n",
    "\n",
    "# Extraire les valeurs de la colonne \"features\" de la première ligne\n",
    "features_values = first_row['features']\n",
    "\n",
    "# Afficher les valeurs\n",
    "print(features_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd80e4",
   "metadata": {},
   "source": [
    "# Création de la fonction de calcul de similarité :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b467ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+-----------+\n",
      "|imdb_title_id_1|imdb_title_id_2|similarity |\n",
      "+---------------+---------------+-----------+\n",
      "|tt0000009      |tt0000009      |1.0        |\n",
      "|tt0000009      |tt0000574      |0.04761905 |\n",
      "|tt0000009      |tt0001892      |0.027777778|\n",
      "|tt0000009      |tt0002101      |0.08695652 |\n",
      "|tt0000009      |tt0002130      |0.032258064|\n",
      "|tt0000009      |tt0002199      |0.033333335|\n",
      "|tt0000009      |tt0002423      |0.04       |\n",
      "|tt0000009      |tt0002445      |0.041666668|\n",
      "|tt0000009      |tt0002452      |0.05       |\n",
      "|tt0000009      |tt0002461      |0.045454547|\n",
      "|tt0000009      |tt0002646      |0.03846154 |\n",
      "|tt0000009      |tt0002844      |0.045454547|\n",
      "|tt0000009      |tt0003014      |0.1        |\n",
      "|tt0000009      |tt0003037      |0.030303031|\n",
      "|tt0000009      |tt0003102      |0.03125    |\n",
      "|tt0000009      |tt0003131      |0.035714287|\n",
      "|tt0000009      |tt0003165      |0.028571429|\n",
      "|tt0000009      |tt0003167      |0.03846154 |\n",
      "|tt0000009      |tt0003419      |0.032258064|\n",
      "|tt0000009      |tt0003471      |0.03448276 |\n",
      "+---------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "# Définir la UDF pour le calcul de similarité de Jaccard\n",
    "def calculate_jaccard_similarity(v1, v2):\n",
    "    if v1 is None or v2 is None:\n",
    "        return 0.0\n",
    "    set1 = set(v1)\n",
    "    set2 = set(v2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    similarity = intersection / union\n",
    "    return float(similarity)\n",
    "\n",
    "# Enregistrer la UDF\n",
    "jaccard_udf = udf(calculate_jaccard_similarity, FloatType())\n",
    "\n",
    "# Jointure croisée pour créer des paires des films\n",
    "similarity_data = vectorized_data.alias('df1').crossJoin(vectorized_data.alias('df2'))\n",
    "\n",
    "# Apply tAppliquer la fonction Jaccard\n",
    "similarity_data = similarity_data.withColumn('similarity', jaccard_udf(col(\"df1.features\"), col('df2.features')))\n",
    "\n",
    "# afficher le resultat:\n",
    "similarity_data.select(col(\"df1.imdb_title_id\").alias(\"imdb_title_id_1\"), \n",
    "                        col(\"df2.imdb_title_id\").alias(\"imdb_title_id_2\"), \n",
    "                        \"similarity\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb5688f",
   "metadata": {},
   "source": [
    "## la fonction de recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52d68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations_from_keywords(keywords, top_n=4):\n",
    "    # Créer un DataFrame avec les mots-clés comme description\n",
    "    keywords_df = spark.createDataFrame([(0, keywords)], [\"imdb_title_id\", \"tags\"])\n",
    "\n",
    "    # Transformer les mots-clés en vecteur TF-IDF\n",
    "    words_data = tokenizer.transform(keywords_df)\n",
    "    filtered_data = remover.transform(words_data)\n",
    "    featurized_data = hashing_tf.transform(filtered_data)\n",
    "    tfidf_keywords = idf_model.transform(featurized_data)\n",
    "    keywords_vector = tfidf_keywords.select(\"features\").collect()[0][0]\n",
    "    keywords_vector_list = keywords_vector.toArray().tolist()\n",
    "\n",
    "    # Ajouter les colonnes de similarité\n",
    "    similarity_data = vectorized_data.withColumn(\"similarity\", jaccard_udf(F.col(\"features\"), F.lit(keywords_vector_list)))\n",
    "\n",
    "    # Trier et sélectionner les films les plus similaires\n",
    "    top_similar_movies = similarity_data.orderBy(F.col(\"similarity\").desc()).select(\"title\", \"original_title\", \"tags\").limit(top_n)\n",
    "\n",
    "    return top_similar_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f433502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation de la fonction\n",
    "keywords = \"action, adventure, hero\"\n",
    "top_similar_movies_df = get_recommendations_from_keywords(keywords, top_n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "147b5185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[title: string, original_title: string, tags: string]\n"
     ]
    }
   ],
   "source": [
    "print(top_similar_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "128be263",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10503.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 12.0 failed 1 times, most recent failure: Lost task 3.0 in stage 12.0 (TID 29) (DESKTOP-LK8RKG4.mshome.net executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1139)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1121)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1568)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1555)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$executeCollect$1(limit.scala:291)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:285)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m top_similar_movies_df\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o10503.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 12.0 failed 1 times, most recent failure: Lost task 3.0 in stage 12.0 (TID 29) (DESKTOP-LK8RKG4.mshome.net executor driver): java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2493)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1139)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1121)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1568)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1555)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.$anonfun$executeCollect$1(limit.scala:291)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\r\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:285)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:286)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\r\n\tat java.io.DataInputStream.readFully(DataInputStream.java:169)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:86)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "top_similar_movies_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f8f081",
   "metadata": {},
   "source": [
    "### On a trouvé des problèmes de configuration et de mémoire pour afficher les résultats, mais la fonction n'affiche aucune erreur. Le problème est juste dans l'affichage du DataFrame des films les plus similaires.Le problème sera resolu si vous avez un pc qui a une grande capacité en ressource mémoire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
